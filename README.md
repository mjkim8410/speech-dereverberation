# ğŸ—£ï¸ Audio Dereverberation with Convâ€‘TasNet

> Endâ€‘toâ€‘end, timeâ€‘domain speech dereverberation at 16â€¯kHz using a Convâ€‘TasNetâ€‘style separator with dilated TCN blocks and SIâ€‘SDR training.

![PyTorch](https://img.shields.io/badge/PyTorch-2.x-red)
![torchaudio](https://img.shields.io/badge/torchaudio-2.x-orange)
![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![License](https://img.shields.io/badge/License-MIT-green)

> âš ï¸ **Project status: Active development**  
> This repository is a work in progress. Interfaces, file paths, and results may change without notice until the first stable release.

## Table of Contents
- [Overview](#overview)
- [Model Architecture](#model-architecture)
- [Loss & Metrics](#loss--metrics)
- [Repository Layout](#repository-layout)
- [Installation](#installation)
- [Data](#data)
- [Training](#training)
- [Results](#results)
- [Roadmap](#roadmap)
- [Citations](#citations)
- [License](#license)
- [Ethics & Intended Use](#ethics--intended-use)

---

## Overview

This repo trains a **singleâ€‘channel speech dereverberation** model directly in the **time domain**. It follows the Convâ€‘TasNet idea: learn an analysis filterbank, enhance with stacked dilated temporal convolution (TCN) blocks, then resynthesize via a synthesis filterbank. Convâ€‘TasNet has proven highly effective for timeâ€‘domain source separation, and the same mechanics transfer to dereverberation.

**Key features**
- **Timeâ€‘domain** endâ€‘toâ€‘end training (no STFT required)
- **Dilated TCN** separator with residual connections
- **Negative SIâ€‘SDR** training objective
- Mixed precision (AMP) and optional **8â€‘bit Adam/AdamW** (bitsandbytes) to cut optimizer memory
- Simple dataset interface for paired *(reverb, clean)* audio segments

---

## Model Architecture

**High level**
1. **Encoder (Conv1d):** learned analysis filterbank  
2. **Separator (TCN stacks):** repeated blocks  
   - 1Ã—1 bottleneck â†’ **depthâ€‘wise dilated Conv1d** (increasing dilation per layer) â†’ **PReLU** â†’ **Global Channel LayerNorm** â†’ 1Ã—1 pointâ€‘wise  
   - residual connection and mask estimation
3. **Masking:** elementâ€‘wise masks (sigmoid) on encoded mixture
4. **Decoder (ConvTranspose1d):** overlapâ€‘add synthesis back to waveform

**Default hyperâ€‘parameters (baseline)**
```python
num_sources=1
encoder_kernel_size=16      # L
encoder_stride=8            # 50% overlap (L/2)
encoder_filters=512         # N
tcn_hidden=128              # B
tcn_kernel_size=3
tcn_layers=8                # per stack (dilations 1..128)
tcn_stacks=3                # number of repeats
causal=False
```

## Loss & Metrics

### Training Loss â€” Negative SIâ€‘SDR

We minimize the **negative Scale-Invariant Signal-to-Distortion Ratio (SIâ€‘SDR)** between the predicted enhanced waveform `Å` and the clean target `s`.

**SI-SDR formula:**

**SIâ€‘SDR(Å, s)** = 10 Â· logâ‚â‚€ ( â€–Î± Â· sâ€–Â² / â€–Å âˆ’ Î± Â· sâ€–Â² )  
â€ƒâ€ƒwhereâ€ƒÎ± = âŸ¨Å, sâŸ© / â€–sâ€–Â²


**Loss function:**

L = âˆ’ mean_batch[SIâ€‘SDR(Å, s)]


This is implemented using **TorchMetrics**â€™ `ScaleInvariantSignalDistortionRatio` class.

#### Optional: Clipping Penalty

An optional **amplitude clipping penalty** (off by default) discourages values of `|Å| > 1`.  
This helps prevent excessive distortion in the generated output.

---

### Validation Metric

- **SIâ€‘SDR (in dB)** â€” Higher is better.
- Optionally, metrics like **PESQ** or **STOI** can be added if ground-truth references are available.


## Repository Layout

```
â”œâ”€â”€ checkpoints_v1/           # Checkpoints for model V1
â”œâ”€â”€ checkpoints_v2/           # Checkpoints for model V2
â”œâ”€â”€ checkpoints_v3/           # Checkpoints for model V3 
â”œâ”€â”€ out/                      # outputs
â”œâ”€â”€ src/                      # Source code: model, training script, etc.
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ dereverb.py
â”‚   â””â”€â”€ (other utils)
â”œâ”€â”€ .gitattributes            
â”œâ”€â”€ .gitignore                
â”œâ”€â”€ LICENSE                   
â”œâ”€â”€ README.md                 
â””â”€â”€ requirements.txt          

```
## Installation

# Python 3.10+ recommended
pip install -r requirements.txt

## Data

This project trains on **paired reverberant-clean speech audio**, where each sample is a 10-second mono waveform pair:

- **Reverberant input** (`x`) â€” artificially reverberated speech  
- **Clean target** (`s`) â€” the original dry (clean) speech

### Source

Clean speech recordings were sourced from the **[LibriVox Project](https://librivox.org/)** â€” a public domain collection of audiobooks read by volunteers.  
LibriVox offers diverse accents, speaker styles, and recording conditions, providing a rich set of high-quality voice data.

### Data Processing
- All audio is converted to **mono**, sampled at **16â€¯kHz**, and segmented into **10-second chunks** in **WAV**
- Original audio had its quality measured by using **DNSMOS** with the results shown below.
<p align="center">
  <img src="dnsmos_plots/p808_density_orig.png"  width="22%">
  <img src="dnsmos_plots/sig_density_orig.png"   width="22%">
  <img src="dnsmos_plots/bak_density_orig.png"   width="22%">
  <img src="dnsmos_plots/ovrl_density_orig.png"  width="22%">
</p>

- Any clip that did not meet the quality thresholdsâ€”**P808 < 3.5, SIG < 3.55, BAK < 4.0, or OVRL < 3.2**â€”was discarded. The score distributions after filtering are shown below.
<p align="center">
  <img src="dnsmos_plots/p808_density.png"  width="22%">
  <img src="dnsmos_plots/sig_density.png"   width="22%">
  <img src="dnsmos_plots/bak_density.png"   width="22%">
  <img src="dnsmos_plots/ovrl_density.png"  width="22%">
</p>

- Few clips that had peaks were also discarded. 
- Remaining audio clips were scaled up right before the point of peaking.
- Audio clips remaining after the processing above was **1,830 hours** in total.

### Reverberation Augmentation

To simulate real-world reverberant environments, each clean audio file was **convolved with room impulse responses (RIRs)** from the  
**[OpenAIR database](https://www.openair.hosted.york.ac.uk/)** â€” a curated collection of acoustically measured spaces.

## Training

Main knobs live at the top of `src/train.py`:

```
NUM_EPOCHS=10
BATCH_SIZE=6
LR=1e-4
SUBSET=0.01          # fraction of dataset sampled per epoch
REVERB_DIRS=[...]
CLEAN_DIR="..."
```
## Results 

*Fill this once training is complete.*

| Model (16â€¯kHz)                | Parameters | Train Data   | Validation SIâ€‘SDR â†‘ |
|------------------------------|------------|--------------|----------------------|
| Convâ€‘TasNet (512â€‘128, 3Ã—8)   | ~8â€¯M       | YourDataset  | XX.Xâ€¯dB              |
| Larger (1024â€‘512, 7Ã—8)       | ~35â€¯M      | YourDataset  | YY.Yâ€¯dB              |


## Roadmap

- [ ] Add multiâ€‘resolution STFT auxiliary loss
- [ ] Add PESQ/STOI evaluation metrics
- [ ] Implement causal (streaming) variant
- [ ] Optional: support DeepSpeed ZeRO / FSDP for larger models

---

## Citations

### Convâ€‘TasNet (architecture inspiration)  
Y. Luo, N. Mesgarani. *Convâ€‘TasNet: Surpassing Ideal Timeâ€‘Frequency Magnitude Estimation for Speech Separation*, IEEE/ACM TASLP, 2019.  
[https://arxiv.org/abs/1809.07454](https://arxiv.org/abs/1809.07454)

### SIâ€‘SDR (objective/metric)  
J. Le Roux, S. Wisdom, H. Erdogan, J. R. Hershey. *SDR â€“ Halfâ€‘Baked or Well Done?*, ICASSP 2019.  
[https://arxiv.org/abs/1811.02508](https://arxiv.org/abs/1811.02508)

### TorchMetrics: SIâ€‘SDR implementation  
[https://torchmetrics.readthedocs.io/en/stable/audio/scale_invariant_signal_distortion_ratio.html](https://torchmetrics.readthedocs.io/en/stable/audio/scale_invariant_signal_distortion_ratio.html)

### DNSMOS (non-intrusive speech-quality metric)
C. K. A. Reddy, V. Gopal, R. Cutler. DNSMOS: A Non-Intrusive Perceptual Objective Speech Quality Metric to Evaluate Noise Suppressors, 
arXiv preprint arXiv:2010.15258, 2020.
[https://arxiv.org/abs/2010.15258](https://arxiv.org/abs/2010.15258)

### PyTorch AMP (mixed precision)  
[https://pytorch.org/docs/stable/amp.html](https://pytorch.org/docs/stable/amp.html)

### bitsandbytes (optional 8â€‘bit optimizers)  
[https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes)

## License

MIT â€” see [LICENSE](./LICENSE).

---

## Ethics & Intended Use

This project is intended for **speech quality enhancement**, such as:

- Removing room reverberation from your own recordings
- Improving accessibility
- Enhancing teleconferencing audio

â— **Please do not** deploy models to circumvent privacy or safety measures, such as:

- Undoing intentional obfuscation (e.g., face blurring or audio masking)
- Processing third-party media without consent



















