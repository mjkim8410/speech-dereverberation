# ðŸ—£ï¸ Audio Dereverberation with Convâ€‘TasNet

> Endâ€‘toâ€‘end, timeâ€‘domain speech dereverberation at 16â€¯kHz using a Convâ€‘TasNetâ€‘style separator with dilated TCN blocks and SIâ€‘SDR training.

![PyTorch](https://img.shields.io/badge/PyTorch-2.x-red)
![torchaudio](https://img.shields.io/badge/torchaudio-2.x-orange)
![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![License](https://img.shields.io/badge/License-MIT-green)

## Table of Contents
- [Overview](#overview)
- [Model Architecture](#model-architecture)
- [Loss & Metrics](#loss--metrics)
- [Repository Layout](#repository-layout)
- [Installation](#installation)
- [Data Preparation](#data-preparation)
- [Training](#training)
- [Evaluation](#evaluation)
- [Export & Inference](#export--inference)
- [Tips for Stability & Speed](#tips-for-stability--speed)
- [Results Placeholder](#results-placeholder)
- [Roadmap](#roadmap)
- [Citations](#citations)
- [License](#license)
- [Ethics & Intended Use](#ethics--intended-use)

---

## Overview

This repo trains a **singleâ€‘channel speech dereverberation** model directly in the **time domain**. It follows the Convâ€‘TasNet idea: learn an analysis filterbank, enhance with stacked dilated temporal convolution (TCN) blocks, then resynthesize via a synthesis filterbank. Convâ€‘TasNet has proven highly effective for timeâ€‘domain source separation, and the same mechanics transfer to dereverberation.

**Key features**
- **Timeâ€‘domain** endâ€‘toâ€‘end training (no STFT required)
- **Dilated TCN** separator with residual connections
- **Negative SIâ€‘SDR** training objective
- Mixed precision (AMP) and optional **8â€‘bit Adam/AdamW** (bitsandbytes) to cut optimizer memory
- Simple dataset interface for paired *(reverb, clean)* audio segments

---

## Model Architecture

**High level**
1. **Encoder (Conv1d):** learned analysis filterbank  
2. **Separator (TCN stacks):** repeated blocks  
   - 1Ã—1 bottleneck â†’ **depthâ€‘wise dilated Conv1d** (increasing dilation per layer) â†’ **PReLU** â†’ **Global Channel LayerNorm** â†’ 1Ã—1 pointâ€‘wise  
   - residual connection and mask estimation
3. **Masking:** elementâ€‘wise masks (sigmoid) on encoded mixture
4. **Decoder (ConvTranspose1d):** overlapâ€‘add synthesis back to waveform

**Default hyperâ€‘parameters (baseline)**
```python
num_sources=1
encoder_kernel_size=16      # L
encoder_stride=8            # 50% overlap (L/2)
encoder_filters=512         # N
tcn_hidden=128              # B
tcn_kernel_size=3
tcn_layers=8                # per stack (dilations 1..128)
tcn_stacks=3                # number of repeats
causal=False

















â”œâ”€ src/
â”‚  â”œâ”€ model.py            # ConvTasNet + TCN blocks + GlobalChannelLayerNorm
â”‚  â”œâ”€ train.py            # training loop
â”‚  â”œâ”€ dereverb.py         # inference / export helpers
â”‚  â””â”€ utils.py            # I/O and helpers (optional)
â”œâ”€ data/
â”‚  â”œâ”€ reverb_chunks/      # reverberant inputs (10 s segments)
â”‚  â””â”€ clean_chunks/       # clean targets (matched names)
â”œâ”€ checkpoints/           # saved model weights (.pth)
â”œâ”€ README.md
â””â”€ LICENSE













